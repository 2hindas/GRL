{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.12.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "bjQemeBnc0HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__version__)\""
      ],
      "metadata": {
        "id": "gz1-gfcWgtYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17157c72-89b7-4db9-a194-2c0d71d9b16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "9nDrhliQcMe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import typing\n",
        "import torch_geometric\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.datasets as datasets\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "TPdmRk21cNoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.Planetoid(\n",
        "    root=\"./\",\n",
        "    name='CiteSeer',\n",
        "    split=\"public\",\n",
        "    transform=torch_geometric.transforms.GCNNorm()\n",
        "  )\n",
        "\n",
        "input_dim = dataset.num_features\n",
        "n_classes = dataset.num_classes\n",
        "\n",
        "print(dataset.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGYkLT-acS5Z",
        "outputId": "2e1105f2-433d-43a2-a9cd-7055ec3e40aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dim: int,\n",
        "      hid_dim: int,\n",
        "      n_classes: int,\n",
        "      n_layers: int,\n",
        "      skip_connections: int,\n",
        "      dropout_ratio: float = 0.3):\n",
        "    super(GAT, self).__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_classes = n_classes\n",
        "    self.n_layers = n_layers\n",
        "    self.skip_connections = skip_connections\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "\n",
        "    modules = []\n",
        "    coefficient_modules = []\n",
        "\n",
        "    if self.n_layers == 1:\n",
        "        modules.append(GATConv(self.input_dim, self.n_classes, feat_drop=dropout_ratio))\n",
        "    else:\n",
        "        modules.append(GATConv(self.input_dim, self.hid_dim, feat_drop=dropout_ratio))\n",
        "        for _ in range(self.n_layers - 2):\n",
        "            coefficient_modules.append(nn.Linear(self.hid_dim, 1))\n",
        "            modules.append(GATConv(self.hid_dim, self.hid_dim, feat_drop=dropout_ratio))\n",
        "        modules.append(GATConv(self.hid_dim, self.n_classes, feat_drop=dropout_ratio))\n",
        "\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    self.softmax = torch.nn.Softmax(dim=1)\n",
        "    self.layers = torch.nn.ModuleList(modules)\n",
        "    self.coefficient_layers = torch.nn.ModuleList(coefficient_modules)\n",
        "\n",
        "  def forward(self, X, A) -> torch.Tensor:\n",
        "    return self.softmax(self.generate_node_embeddings(X, A))\n",
        "\n",
        "  def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
        "\n",
        "    # No skip connections\n",
        "    if self.skip_connections == 0:\n",
        "        output = X\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i < len(self.layers) - 1:\n",
        "                output = F.dropout(F.relu(layer(output, A)),\n",
        "                                   p=self.dropout_ratio,\n",
        "                                   training=self.training)\n",
        "            else:\n",
        "                output = layer(output, A)\n",
        "        return output\n",
        "\n",
        "    else:\n",
        "        output = X\n",
        "        skip = None\n",
        "        for i, layer in enumerate(self.layers):\n",
        "\n",
        "            # first layer\n",
        "            if i == 0:\n",
        "                output = F.dropout(F.relu(layer(output, A)),\n",
        "                                   p=self.dropout_ratio,\n",
        "                                   training=self.training)\n",
        "                \n",
        "                # store first layer for skip connection\n",
        "                skip = output\n",
        "\n",
        "            # last layer\n",
        "            elif i == len(self.layers) - 1:\n",
        "                output = layer(output, A)\n",
        "\n",
        "            # hidden layers\n",
        "            else:\n",
        "                \n",
        "                # Normal skip connections\n",
        "                if self.skip_connections == 1:\n",
        "\n",
        "                    output = F.dropout(F.relu(layer(output, A)),\n",
        "                                       p=self.dropout_ratio,\n",
        "                                       training=self.training) + skip\n",
        "\n",
        "                # Weighted skip connections\n",
        "                elif self.skip_connections == 2:\n",
        "\n",
        "                    weight = F.dropout(self.sigmoid(self.coefficient_layers[i-1](output)),\n",
        "                                       p=self.dropout_ratio,\n",
        "                                       training=self.training)\n",
        "                    output = (1 - weight) * F.dropout(F.relu(layer(output, A)),\n",
        "                                                      p=self.dropout_ratio,\n",
        "                                                      training=self.training) + weight * skip\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "rQ1ma-fAcrLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    model,\n",
        "    data,\n",
        "    mask\n",
        "):\n",
        "    model.eval()\n",
        "    predictions = torch.argmax(model(data.x, data.edge_index), dim=1)\n",
        "    return torch.sum(predictions[mask] == data.y[mask]) / torch.sum(mask)\n",
        "\n",
        "def train(\n",
        "    params: typing.Dict\n",
        ") -> torch.nn.Module:\n",
        "\n",
        "    # set GPU or CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # load dataset\n",
        "    data = dataset.data\n",
        "    data = data.to(device)\n",
        "\n",
        "    model = GAT(\n",
        "        params[\"input_dim\"], \n",
        "        params[\"hid_dim\"],\n",
        "        params[\"n_classes\"],\n",
        "        params[\"n_layers\"],\n",
        "        params[\"skip_connections\"],\n",
        "        params[\"dropout\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()), \n",
        "        lr=params[\"lr\"],\n",
        "        weight_decay=params[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(params[\"epochs\"]):\n",
        "\n",
        "        model.train()\n",
        "        predictions = model(data.x, data.edge_index).to(device)\n",
        "\n",
        "        loss = loss_criterion(predictions[data.train_mask], torch.nn.functional.one_hot(\n",
        "            data.y[data.train_mask], params[\"n_classes\"]).float())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        val_accuracy = evaluate(model, data, data.val_mask)\n",
        "        val_accuracies.append(val_accuracy.item())\n",
        "\n",
        "        if len(val_accuracies) >= params[\"max_patience\"]:\n",
        "            decreasing = True\n",
        "            for i in range(1, params[\"max_patience\"]):\n",
        "                decreasing &= (val_accuracies[-i] < val_accuracies[-i-1])\n",
        "            if decreasing:\n",
        "                break  # Early stop\n",
        "\n",
        "    test_accuracy = evaluate(model, data, data.test_mask).item()\n",
        "\n",
        "    return model, test_accuracy"
      ],
      "metadata": {
        "id": "Ob1Qgp26fItN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMETERS\n",
        "\n",
        "training_params = {\n",
        "    \"lr\": 0.005,                # learning rate\n",
        "    \"weight_decay\": 0.005,      # weight_decay\n",
        "    \"epochs\": 100,              # total training epochs\n",
        "    \"max_patience\": 5,          # early stopping time\n",
        "    \"hid_dim\": 128,             # hidden layer size\n",
        "    \"n_layers\": None,           # number of attention layers\n",
        "    \"skip_connections\": 0,      # 0 for no connections, 1 for normal, 2 for weighted connections\n",
        "    \"dropout\": 0.5,             # dropout rate\n",
        "    \"input_dim\": input_dim,     # input feature size\n",
        "    \"n_classes\": n_classes,     # number of classes\n",
        "}"
      ],
      "metadata": {
        "id": "ikFlETr8fe1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# TRAIN VARIOUS MODELS\n",
        "\n",
        "num_layers = [1, 3, 6, 10, 20]\n",
        "skip_connections = [0, 1, 2]\n",
        "N = 10\n",
        "\n",
        "accuracies = np.zeros((len(skip_connections), len(num_layers)))\n",
        "\n",
        "for j, num_layer in enumerate(num_layers):\n",
        "    for i, skip_connection in enumerate(skip_connections):\n",
        "        \n",
        "        training_params[\"skip_connections\"] = skip_connection\n",
        "        training_params[\"n_layers\"] = num_layer\n",
        "\n",
        "        test_accuracies = []\n",
        "\n",
        "        for _ in range(0, N):\n",
        "            model, test_acc = train(training_params)\n",
        "            test_accuracies += [test_acc]\n",
        "\n",
        "        print(f\"Mean: {np.round(np.mean(test_accuracies), 3)},  Skip connection: {skip_connection},  num_layers: {num_layer}\")\n",
        "\n",
        "        accuracies[i][j] = np.mean(test_accuracies)\n",
        "\n",
        "print(accuracies)"
      ],
      "metadata": {
        "id": "se_Zs-shgJ9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dimension_reduction(model) -> pd.DataFrame:\n",
        "    model.eval()\n",
        "    embedding = model.generate_node_embeddings(dataset.data.x, dataset.data.edge_index)[dataset.data.test_mask]\n",
        "    X_embedded = TSNE(n_components=2).fit_transform(embedding.detach().cpu())\n",
        "    df = pd.DataFrame(X_embedded)\n",
        "    df.insert(2, 'labels', dataset.data.y[dataset.data.test_mask].detach().cpu())\n",
        "    return df"
      ],
      "metadata": {
        "id": "NN1VDQdDfk8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise(feature_dict: typing.Dict, name: str) -> None:\n",
        "    colors = {0:'mediumaquamarine', 1:'dodgerblue', 2:'seagreen', 3:'gold', 4:'orangered', 5:'orchid', 6:'teal'}\n",
        "\n",
        "    plt.figure(figsize=(22, 4))\n",
        "    \n",
        "    for i, key in enumerate(feature_dict):\n",
        "        plt.subplot(1, len(feature_dict), i+1)\n",
        "        data = feature_dict[key]\n",
        "        plt.scatter(data[0], data[1], c=data['labels'].map(colors))\n",
        "        plt.title(key)\n",
        "\n",
        "    plt.savefig(name + \".png\", format = 'png', dpi=500)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RfnvBXCdf_8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALISE EMBEDDINGS\n",
        "\n",
        "for i in range(0, 3):\n",
        "\n",
        "    training_params[\"skip_connections\"] = i\n",
        "\n",
        "    training_params[\"n_layers\"] = 1\n",
        "    A, _ = train(training_params)\n",
        "\n",
        "    training_params[\"n_layers\"] = 3\n",
        "    B, _ = train(training_params)\n",
        "\n",
        "    training_params[\"n_layers\"] = 7\n",
        "    C, _ = train(training_params)\n",
        "\n",
        "    training_params[\"n_layers\"] = 10\n",
        "    D, _ = train(training_params)\n",
        "\n",
        "    training_params[\"n_layers\"] = 20\n",
        "    E, _ = train(training_params)\n",
        "\n",
        "    feature_dict = {\n",
        "        \"1 layer\": dimension_reduction(A),\n",
        "        \"3 layers\": dimension_reduction(B),\n",
        "        \"7 layers\": dimension_reduction(C),\n",
        "        \"10 layers\": dimension_reduction(D),\n",
        "        \"20 layers\": dimension_reduction(E),\n",
        "    }\n",
        "\n",
        "    visualise(feature_dict, f\"skip_connection_{i}\")"
      ],
      "metadata": {
        "id": "NSQ5GFztr0cb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}